---
Author: 
tags: []
date:
---

## VLLM

Принимают на вход картинку и умеют отвечать по ней на вопросы на естественном языке.
1. Qwen2-VL (сначала 2B тестим)
2. MiniCPM-V 2.6
3. RuDOLPH
4. LayoutLM2, 3, XML
5. deepvk/llava-saiga-8b  

Интересные модели, но слишком тяжелые:
- Pixtral-Large-Instruct-2411([ссылка](https://huggingface.co/mistralai/Pixtral-Large-Instruct-2411)) - 124B параметров, 52 файла saаetensors по 5 Гб каждый, итого где то ~ 260 Gb RAM для запуска.
Это примерно: 4 А100 по 80 Gb RAM.
Особая лицензия ([ссылка](https://mistral.ai/licenses/MRL-0.1.md))

## Перспективные LLM без Vision encoder

Не умеют отвечать на вопросы по картинкам. Нужно добавлять Vision encoder и доучивать до VLLM.
Скорее полезная база для VLLM:
1. RuQwen2.5-3B-Instruct-AWQ ([ссылка](https://huggingface.co/FractalGPT/RuQwen2.5-3B-Instruct-AWQ))
2. RuadaptQwen-2.5-32B-Instruct ([ссылка](https://t.me/ruadaptnaya/7))
3. Qwen2.5-Coder-32B-Instruct ([ссылка](https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct))