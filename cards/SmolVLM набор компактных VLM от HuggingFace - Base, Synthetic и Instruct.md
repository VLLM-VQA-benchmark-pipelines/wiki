---
Author:
  - –û–≤—á–∏–Ω–Ω–∏–∫–æ–≤–∞ –Æ–ª–∏—è
tags:
  - smolvlm
date: 0224-12-06
---

–ü–æ—Å—Ç –≤ Telegram ([link](https://t.me/c/2429357431/63/701)) 
@ai_machinelearning_big_data

[SmolVLM](https://huggingface.co/collections/HuggingFaceTB/smolvlm-6740bd584b2dcbf51ecb1f39) - —Å–µ—Ä–∏—è –∫–æ–º–ø–∞–∫—Ç–Ω—ã—Ö VLM —Å 2 –º–ª—Ä–¥. –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –æ—Ç–ª–∏—á–∞—é—â–∏—Ö—Å—è –≤—ã—Å–æ–∫–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏ –∏ –º–æ–≥—É—Ç –±—ã—Ç—å —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏.

–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ SmolVLM –æ—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ Idefics3, —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –æ—Ç–ª–∏—á–∏—è–º–∏:
üü¢–í –∫–∞—á–µ—Å—Ç–≤–µ —è–∑—ã–∫–æ–≤–æ–π –æ—Å–Ω–æ–≤—ã –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è SmolLM2 1.7B –≤–º–µ—Å—Ç–æ Llama 3.1 8B;
üü¢–í–∏–∑—É–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Å–∂–∏–º–∞–µ—Ç—Å—è –≤ 9 —Ä–∞–∑ —Å –ø–æ–º–æ—â—å—é —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ pixel shuffle, –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å 4-–∫—Ä–∞—Ç–Ω—ã–º —Å–∂–∞—Ç–∏–µ–º –≤ Idefics3;
üü¢–ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –ø–∞—Ç—á–∏  —Ä–∞–∑–º–µ—Ä–æ–º 384x384 –ø–∏–∫—Å–µ–ª–µ–π, –∞ –Ω–µ 364x364;
üü¢–í–∏–∑—É–∞–ª—å–Ω–∞—è –æ—Å–Ω–æ–≤–∞ –º–æ–¥–µ–ª–∏ –∏–∑–º–µ–Ω–µ–Ω–∞ –Ω–∞ shape-optimized SigLIP —Å –ø–∞—Ç—á–∞–º–∏ 384x384 –ø–∏–∫—Å–µ–ª–µ–π –∏ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–º–∏ –ø–∞—Ç—á–∞–º–∏ 14x14;
üü¢–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ SmolLM2 –±—ã–ª–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–æ –¥–æ 16 —Ç—ã—Å. —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ —Ä–∞–±–æ—Ç—ã —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏.

–ú–æ–¥–µ–ª—å –∫–æ–¥–∏—Ä—É–µ—Ç –∫–∞–∂–¥—ã–π –ø–∞—Ç—á –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è 384x384 –≤ 81 —Ç–æ–∫–µ–Ω, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –µ–π –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤—Å–µ–≥–æ 1.2 —Ç—ã—Å. —Ç–æ–∫–µ–Ω–æ–≤, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ Qwen2-VL –∏—Å–ø–æ–ª—å–∑—É–µ—Ç 16 —Ç—ã—Å. —Ç–æ–∫–µ–Ω–æ–≤. –≠—Ç–æ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–π —Å–∫–æ—Ä–æ—Å—Ç–∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–≤ 3,3-4,5 —Ä–∞–∑–∞) –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–≤ 7,5-16 —Ä–∞–∑) –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å Qwen2-VL.

–î–ª—è —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ–π —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ SmolVLM –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å [transformers –∏ TRL](https://huggingface.co/blog/smolvlm#fine-tuning). –†–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞–º–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω [–±–ª–æ–∫–Ω–æ—Ç](https://github.com/huggingface/smollm/blob/main/finetuning/Smol_VLM_FT.ipynb) –¥–ª—è —Ñ–∞–π–Ω—Ç—é–Ω–∞ –Ω–∞ VQAv2 —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LoRA, QLoRA –∏–ª–∏ –ø–æ–ª–Ω–æ–π —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏. SmolVLM –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω —Å TRL –¥–ª—è DPO —á–µ—Ä–µ–∑ CLI.

‚ö†Ô∏è –ü—Ä–∏ batch sizes=4 –∏ 8-–±–∏—Ç–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–µ QLoRA —Ñ–∞–π–Ω—Ç—é–Ω –ø–æ—Ç—Ä–µ–±–ª—è–µ—Ç –æ–∫–æ–ª–æ ~16 GB VRAM

üìå–õ–∏—Ü–µ–Ω–∑–∏—Ä–æ–≤–∞–Ω–∏–µ: ¬†Apache 2.0

üü°[–°—Ç–∞—Ç—å—è –Ω–∞ HF](https://huggingface.co/blog/smolvlm)
üü°[–ù–∞–±–æ—Ä –º–æ–¥–µ–ª–µ–π](https://huggingface.co/collections/HuggingFaceTB/smolvlm-6740bd584b2dcbf51ecb1f39)
üü°[Demo](https://huggingface.co/spaces/HuggingFaceTB/SmolVLM)


![](files/SmolVLM-20241206.jpg)



