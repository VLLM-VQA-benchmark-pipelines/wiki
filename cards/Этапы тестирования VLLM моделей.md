---
Author:
  - Ширяев Антон
  - Изюмова Анастасия
tags:
  - промпт_инеженеринг
  - бенчмарк
date: 2024-12-05
---
# Этап 1 Тестирование VLLM моделей и  отбор промптов для них

1. **Модель-ответчик**: выбираем модель, которая будет отвечать на вопросы по картинкам.
* это все VLLM модели, которые мы планируем тестировать нашим бенчмарком ([ссылка](../cards/Выбранные%20модели%20для%20оценки%20бенчмарком.md)).
* например, `Qwen2-VL-2B`.

2. **Модель-генератор промпта**: выбираем большую модель (`GPT4o`, `DeepSeek`, `Qwen2-VL-72B`), которая будет предлагать нам пропмт для **модели-ответчика**.

* Это любые Большие VLLM, доступные по API или через web-интерфейс.
* Выбрать 5-6 моделей-лидеров на сайте ([ссылка](https://lmarena.ai/)) и или любые другие представляющие интерес и работать с ними.

3. **Данные:** датасеты из этапа 1 ([ссылка](../cards/Датасеты%20для%20бенчмарка.md)).

## Отбор VLLM модели

* Прочитали про новую перспективную VLLM-модель
* Хотим предварительно оценить ее возможности

**Примерный порядок:**
* идем в веб-демо или API модели
* берем 1-3 картинки из датасета
* задаем ей вопросы
* смотрим ответы
* думаем стоит ли работать с этой моделью
Если она имеет некоторый потенциал, переходим к этапу подбора промптов для нее.

## Подбор промптов для VLLM модели

**Примерный порядок:**
* идем в веб-демо или API **модели-генератора промптов**
* пишем ей **наш промпт** с просьбой сгенерировать оптимальный промпт для **модели-ответчика**
Промпт на всякий случай сохраняем себе, возможно пригодится при работе с другими **моделями генераторами промптов**.
* получаем промпт для **модели-ответчика**
* переходим к **мини бенчмарку** оценивающему ответы **модели-ответчика** на датасетах этапа 1 ([ссылка](../cards/Датасеты%20для%20бенчмарка.md)).
* записываем в него тестируемый промпт, запускаем быстрый мини бенчмарк и смотрим полученные метрики
* если метрики имеют потенциал, сохраняем промпт.
* если нет, пробуем улучшить данный промпт
1. Можем написать новый промпт для **модели-генератора промптов**, указав ей полученные метрики на **мини бенчмарке** от предложенного ей ранее промпта. И попросить ее оптимизировать предложенный ей ранее промпт. Повторяем этот процесс циклически, смотрим улучшаются ли метрики на мини бенчмарке?
2. Использовать специальные инструменты для оптимизации промпта, например [textgrad](https://github.com/zou-group/textgrad) .
3. Применять любые техники и подходы

Записываем лучшие промпты в csv-файлы.
Название файла указываем в виде:
```
<название_модели_ответчика>_<название_модели_генератора_промпта>.csv
```

Например, мы получили "Qwen2-VL-2B_sep_GPT4o.csv", вида:

| doc type question type | optimal prompt                | cer |
| ---------------------- | ----------------------------- | --- |
| Паспорт Имя            | "Текст опт. промпта от GPT4o" | 0.1 |
| Паспорт Номер паспорта | "Текст опт. промпта от GPT4o" | 0.2 |
| СНИЛС                  | "Текст опт. промпта от GPT4o" | 0.2 |
И еще 5 подобных файлов от других **моделей-генераторов-промпта**.
* "Qwen2-VL-2B_sep_Qwen2-VL-72B.csv"
* "Qwen2-VL-2B_sep_DeepSeek.csv"
...
* и т.д.

# Этап 2 Выбираем оптимальный набор промптов для каждой модели

С этапа 1 для каждой тестируемой в бенчмарке модели ([ссылка](../cards/Выбранные%20модели%20для%20оценки%20бенчмарком.md)) у нас есть набор промптов от различных **моделей-генераторов-промпта**:

* "Qwen2-VL-2B_sep_GPT4o.csv"
* "Qwen2-VL-2B_sep_Qwen2-VL-72B.csv"
* "Qwen2-VL-2B_sep_DeepSeek.csv"
...
* и т.д.

На этом этапе мы отбираем **лучшие промпты** из всех возможных вариантов для выбранной **VLLM-модели** на больших датасетах, сравнивая их на "хорошей" статистике.

**Данные:** датасеты из этапа 2 ([ссылка](../cards/Датасеты%20для%20бенчмарка.md)).

На этом этапе бенчмарка оцениваем:
* имеющиеся наборы промптов (от **моделей-генераторов промптов**) для извлечения нужной информации из документов для каждой тестируемой в бенчмарке модели ([ссылка](../cards/Выбранные%20модели%20для%20оценки%20бенчмарком.md))
* оцениваем по метрикам корректность извлекаемых данных
* отбираем лучшие промпты для каждой модели ([ссылка](../cards/Выбранные%20модели%20для%20оценки%20бенчмарком.md)) из всех представленных вариантов

Мы хотим, чтобы у **VLLM-модели** не было трудностей с чтением информации с документа.

# Этап 3 Исследуем работу VLLM-модели на всевозможных данных

На этом этапе исследуем работу **VLLM-модели** с одним оптимально подобранным для нее набором промптов(на этапе 2) на максимально разнообразных данных.

Исследуем возможности **VLLM-модели**, знакомим ее со всеми возможными данными, создаем ей различные трудности: разрешение, аугментации и т.д.

**Данные:** датасеты из этапа 3 ([ссылка](../cards/Датасеты%20для%20бенчмарка.md)).

На этом этапе бенчмарка оцениваем:
* работу **VLLM-модели** ([ссылка](../cards/Выбранные%20модели%20для%20оценки%20бенчмарком.md)) на максимально разнообразных данных
* исследуем туда ли смотрит модель?
* правильно ли распознается текст и каких ситуациях?
* ищем слабые места и точки роста
* обсуждаем со стейкхолдером имеющиеся точки роста и пути их достижения