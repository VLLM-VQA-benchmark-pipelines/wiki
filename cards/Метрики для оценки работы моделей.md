---
Author:
  - Ширяев Антон
  - Капустин Евгений
tags:
  - метрики
  - OCR
  - NLP_natural_language_processing
date: 2024-11-11
---
Для оценки качества работы OCR (распознавания текста) моделей обычно используют несколько метрик, которые помогают оценить точность и полноту распознавания текста, а также учитывают различные аспекты ошибок. Вот основные из них:

1. **Word Accuracy Rate (WAR)** или **Word Recognition Rate (WRR)** – это метрика, оценивающая точность распознавания слов. Выражается как процент правильно распознанных слов среди всех слов в тексте:
   $$WAR = \frac{\text{Количество правильно распознанных слов}}{\text{Общее количество слов}}$$

2. **Character Accuracy Rate (CAR)** – метрика, оценивающая точность распознавания на уровне символов. Она показывает, какой процент символов распознан правильно:
   $$CAR = \frac{\text{Количество правильно распознанных символов}}{\text{Общее количество символов}}$$

3. **Word Error Rate (WER)** – более строгая метрика, которая учитывает количество вставок, удалений и замен символов для достижения точного совпадения. WER показывает процент ошибок по отношению к исходному тексту:
  $$WER = \frac{\text{S + D + I}}{\text{N}}$$
   где:
   - \(S\) – количество замен (substitutions),
   - \(D\) – количество удалений (deletions),
   - \(I\) – количество вставок (insertions),
   - \(N\) – общее количество слов в эталонном тексте.

4. **Character Error Rate (CER)** – аналог WER, но на уровне символов. CER полезен для точной оценки работы модели при распознавании языков, в которых могут быть ошибки на уровне символов (например, для восточноазиатских языков):
   $$CER = \frac{\text{S + D + I}}{\text{Количество символов в эталонном тексте}}$$
**Примечание:**
>Если Character Error Rate больше 1 (или 100%), это означает, что количество ошибок в распознанном тексте превышает общее количество символов в эталонном тексте. Это может произойти в следующих случаях:

5. **Edit Distance** или **Levenshtein Distance** – это количество изменений (вставок, удалений и замен), которые нужно сделать, чтобы преобразовать распознанный текст в исходный. Этот показатель можно использовать как дополнительную метрику для оценки ошибок OCR.

6. **Normalized Edit Distance (NED)** – нормализированная редакционная дистанция, которая выражается в виде процента или доли от общего числа символов. Этот показатель полезен, если необходимо сравнивать модели на текстах разной длины:
   $$NED = \frac{\text{Edit Distance}}{\text{Максимальное число символов в эталонном или предсказанном тексте}}$$

7. BLEU (Bilingual Evaluation Understudy) — это метрика, разработанная для оценки качества машинного перевода, но она также применяется в других задачах, где важно оценить схожесть текста, например, в OCR или текстовых генерациях. BLEU оценивает, насколько сгенерированный текст (гипотеза) близок к эталонному (или эталонным) тексту, на основе совпадений фраз разной длины.

### 1. **Основная идея BLEU**
BLEU сравнивает гипотезу с эталонным текстом на уровне n-грамм (групп из n последовательных слов), чтобы понять, насколько они совпадают. Чем больше совпадений n-грамм между гипотезой и эталоном, тем выше оценка BLEU. Оценка BLEU принимает значения от 0 до 1 (или от 0 до 100 при выражении в процентах), где 1 означает полное совпадение с эталонным текстом.

### 2. **N-граммы и точность совпадения**
BLEU использует несколько уровней n-грамм для оценки. Обычно берут от 1 до 4, чтобы учесть совпадения как отдельных слов, так и коротких фраз. Например, если гипотеза состоит из "This is a test", и эталон — "This is a simple test", BLEU будет проверять:

   - **1-граммы** (одиночные слова): "This", "is", "a", "test".
   - **2-граммы** (пары слов): "This is", "is a", "a test".
   - **3-граммы** и **4-граммы** для совпадения коротких фраз.

BLEU вычисляет количество совпадающих n-грамм в гипотезе по сравнению с эталоном и делит это количество на общее число n-грамм в гипотезе. Таким образом, BLEU учитывает **точность совпадений n-грамм**.

### 3. **Метрический расчет: precision и модифицированная точность**
BLEU использует так называемую модифицированную точность (precision) для каждого n-грамма, чтобы избегать избыточных совпадений. Например, если слово "the" встречается 3 раза в гипотезе и только 1 раз в эталоне, BLEU засчитает его только один раз, чтобы избежать завышенной оценки.

### 4. **Brevity Penalty (Штраф за краткость)**
BLEU также применяет штраф за краткость, чтобы избегать ситуаций, когда модель выдает слишком короткие гипотезы, совпадающие по n-граммам, но не по длине. Этот штраф рассчитывается как:

   $$
   BP = \begin{cases} 
      1, & \text{если длина гипотезы } > \text{длина эталона} \\ 
      e^{1 - \frac{\text{длина эталона}}{\text{длина гипотезы}}}, & \text{иначе} 
   \end{cases}
$$

BLEU итогово умножает получившуюся оценку на штраф за краткость, если гипотеза короче эталона.

### 5. **Итоговая формула BLEU**
Итоговая формула BLEU выглядит так:
$$
BLEU = BP \times \exp\left(\sum_{n=1}^{N} w_n \log p_n\right)
$$

где:
- $BP$ — штраф за краткость,
- $p_n$ — модифицированная точность для n-грамм,
- $w_n$ — вес для каждой n-граммы (обычно равные доли для всех n-грамм).

### 6. **Особенности и ограничения BLEU**
BLEU имеет ряд преимуществ и ограничений:

   - **Преимущества**:
      - Простая и эффективная метрика для оценки схожести текста.
      - Хорошо работает для длинных текстов.
   
   - **Ограничения**:
      - Не учитывает семантическое сходство: BLEU оценивает только точные совпадения, не анализируя смысл.
      - Чувствительна к словарным и синтаксическим различиям, что иногда приводит к низким оценкам, даже если текст стилистически схож.

### 7. **Применение BLEU в Python**
Для расчета BLEU можно использовать библиотеку `nltk` или `sacrebleu`.

Пример с использованием `sacrebleu`:

```python
import sacrebleu

# Пример эталонного и гипотезного текста
reference = ["This is the reference text"]
hypothesis = ["This is the hypothesis text"]

# Вычисление BLEU
bleu_score = sacrebleu.corpus_bleu(hypothesis, [reference])
print("BLEU Score:", bleu_score.score)
```

Таким образом, BLEU — полезная метрика для задач, где важен структурный и лексический уровень совпадения, и она широко используется в NLP для быстрой оценки качества текста.

## 8. ANLS

ANLS (Average Normalized Levenshtein Score) — это метрика, которая используется для того, чтобы понять, насколько точно модель распознаёт текст, например, в задачах, где нужно извлечь текст из изображений (например, при OCR).

Чтобы объяснить проще:
1. Levenshtein Distance — это способ измерить, насколько две строки текста отличаются друг от друга. Чем больше изменений нужно сделать (добавить, удалить или заменить символы), чтобы одна строка превратилась в другую, тем больше это расстояние.    
2. Нормализация означает, что мы делим это расстояние на длину самой длинной строки, чтобы сравнение не зависело от длины текста. Например, если одна строка очень короткая, а другая длинная, то просто считать их разницу в символах было бы нечестно.    
3. ANLS — это просто среднее значение нормализованного расстояния Левенштейна для всех строк в наборе. То есть мы вычисляем, как точно модель распознала каждую строку, и затем усредняем результаты.    

Пример:

- У вас есть эталонный текст (правильный) и текст, который модель распознала.    
- Вы считаете, сколько изменений нужно сделать, чтобы превратить распознанный текст в правильный.    
- Затем делаете это для всех строк, а в конце берёте среднее значение.

Таким образом, ANLS помогает оценить, насколько точно модель восстанавливает текст, учитывая даже небольшие ошибки, такие как неправильные буквы или пропуски. Чем меньше значение ANLS, тем лучше модель распознала текст.


![](../files/Датасеты%20Метрики-20241120.png)

## Реализованные метрики в бенчмарке Антона

- WER (библиотеки jiwer)    
- CER (библиотеки jiwer)    
- BLEU (библиотеки sacrebleu)    

GitLab группа: [https://gitlab.com/document_vqa](https://gitlab.com/document_vqa)