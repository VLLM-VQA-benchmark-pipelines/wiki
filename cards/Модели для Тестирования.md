---
Author:
  - Ширяев Антон
  - Изюмова Анастасия
  - Овчинникова Юлия
tags:
  - VLLM
date: 2024-11-21
---
Здесь рассматриваем все полезные для наших задач модели.
# VLLM

`VLLM` - Vision Large Language Models.
Эти модели принимают на вход картинку и умеют отвечать по ней на вопросы на естественном языке.
Архитектурно:
* `LLM`
* некоторый `Vision encoder`
`Vision Encoder` обычно тоже архитектуры трансформер, и выдает из себя последовательность `vision-токенов`, которая кодирует информацию из картинки.

Подробнее про VLLM тут ([ссылка](Мультимодальные%20модели%20и%20LLM.md)).
## Тестируем модели:
1. Qwen2-VL (сначала 2B, затем 7B)
от разработчиков:
* [ссылка на статью](https://arxiv.org/pdf/2409.12191)
* [ссылка на блог](https://qwenlm.github.io/blog/qwen2-vl/)
* [ссылка на HuggingFace](https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct)
* [ссылка на GitHub](https://github.com/QwenLM/Qwen2-VL) 
* [ссылка на Docker image](https://github.com/QwenLM/Qwen2-VL?tab=readme-ov-file#-docker) [ссылка на Dockerfile-cu121](https://github.com/QwenLM/Qwen2-VL/blob/main/docker/Dockerfile-cu121)
от меня:
* репозиторий с запуском и тестами модели [ссылка](https://gitlab.com/document_vqa/qwen2-vl)
* код для ответов на вопросы моделью по документам [ссылка](https://gitlab.com/document_vqa/qwen2-vl/-/blob/main/src/run_predict.py?ref_type=heads)
* [ссылка на Dockerfile-cu124](https://github.com/QwenLM/Qwen2-VL/blob/main/docker/Dockerfile-cu121) [инструкции по сборке и запуску контейнера](https://gitlab.com/document_vqa/qwen2-vl#docker-контейнер)
2. MiniCPM-V 2.6 [ссылка](https://huggingface.co/openbmb/MiniCPM-V-2_6) Лицензия на модель не свободная.
от меня:
* репозиторий с запуском и тестами модели [ссылка](https://gitlab.com/document_vqa/minicpm-v-2-6)
3. RuDOLPH [ссылка на конспект](../../../cards/Конспект%20Андрей%20Кузнецов%20Мультимодальные%20модели,%20как%20научить%20языковые%20модели%20работать%20не%20только%20с%20текстом.md)
4. LayoutLM2, 3, XML
5. deepvk/llava-saiga-8b
### Потенциально:
* Griffon-G [ссылка](https://paperswithcode.com/paper/griffon-g-bridging-vision-language-and-vision) [ссылка на github](https://github.com/jefferyzhan/griffon) обещают выложить модель, но пока ее нет.

## Интересные VLLM, но слишком тяжелые:
- Pixtral-Large-Instruct-2411([ссылка](https://huggingface.co/mistralai/Pixtral-Large-Instruct-2411)) - 124B параметров, 52 файла saаetensors по 5 Гб каждый, итого где то ~ 260 Gb RAM для запуска.
Это примерно: 4 А100 по 80 Gb RAM.
Особая лицензия ([ссылка](https://mistral.ai/licenses/MRL-0.1.md))

## Обучение своей VLLM под задачу

Интеренcное видео ([ссылка](https://www.youtube.com/watch?v=T7sxvrJLJ14&ab_channel=AIEngineer)) про обучение компактной быстрой 2B VLM, которая оказалась лучшей в своем классе. [ссылка на источник](https://t.me/ai_newz/3443)

**Модель:**
* LLM: Phi1.6B 
* Vision encoder: SigLIP 400M

**Cинтетический датасет:**
* на задачу LNQA (Localized Narratives Question Answering) с вопросами-ответами по картинкам
* 300к пар.

**Итог:**
* Получилась сильная и шустрая модель.
* Правильные данные решают.

**P.S.:** Автор получил 5M долларов и сроит стартап `moondream.ai` по обучению компактных моделей.

**Материалы**:
- Github [ссылка](https://github.com/vikhyat/moondream)
- Demo [ссылка](https://huggingface.co/spaces/vikhyatk/moondream2)
- Blogpost про синтетический QA датасет [ссылка](https://vikhyat.net/posts/2024-08-17-lnqa.html)
- Видео [ссылка](https://www.youtube.com/watch?v=T7sxvrJLJ14)

> Кто за то, чтоб повторить? =)
## Перспективные LLM без Vision encoder

* Не умеют отвечать на вопросы по картинкам.
* Нужно добавлять `Vision encoder` и доучивать до `VLLM`.

Скорее полезная база для `VLLM`:
1. RuQwen2.5-3B-Instruct-AWQ ([ссылка](https://huggingface.co/FractalGPT/RuQwen2.5-3B-Instruct-AWQ))
2. RuadaptQwen-2.5-32B-Instruct ([ссылка](https://t.me/ruadaptnaya/7))
3. Qwen2.5-Coder-32B-Instruct ([ссылка](https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct))