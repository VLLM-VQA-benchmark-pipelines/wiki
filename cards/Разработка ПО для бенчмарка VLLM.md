---
Author:
  - Ширяев Антон
  - Изюмова Анастасия
  - Овчинникова Юлия
tags:
  - tools
  - задача
date: 2024-11-10
---
## Задачи

Запуск бенчмарка:
- Запустить бенчмарк с парой датасетов на которых он умеет работать какой то из моделей которые он поддерживает.    
- Запустить маленьком кусочке датасета (из 20 картинок и вопросов по ним)    

Поиск подходящего формата датасета:
- Посмотрим готовые бенчмарки    
- Посмотреть форматы датасетов которые они поддерживает    
- Поискать среди этих форматов наиболее подходящие под нашу задачу    
- Если такой формат найден то используем его!    

Поиск способа добавления нашего формата датасета:
- Если не найден, то делаем свой кастомный.    
- Надо понять какой класс написать для того, чтобы этот бенчмарк смог работать с датасетом нашего формата.    

Какие метрики поддерживает данный бенчмарк?
- Посмотреть какие метрики поддерживаются    
- Есть ли полезные для наших задач    

Запуск наших моделей на бенчмарке:
- Смотрим какие поддерживаются и тестируем их на мини кусочках датасета(убеждаемся что все работает)    
- Ищем способ как добавить свою новую модель в данный бенчмарк. Есть какой то класс, описывающий предикт моделью на картинке при заданном вопросе. 
- Реализуем свой класс поддерживающий каждую из моделей.    

## Фреймворки
### Hugging Face    

1. Позволяет создать датасет	    
2. Позволяет опубликовать свой датасет	    
3. Инструменты для оценивания моделей на датасет.	    
4. Умеет считать метрики WER и CER

### vllm

Фреймворк для оптимизированного инференса и сервинга моделей    

Полезные ссылки о vllm ([ссылка](https://habr.com/ru/companies/mts_ai/articles/791594/) ускорение в 20 раз, сравнение инструментов сервинга LLM [ссылка](https://vc.ru/ai/1247008-sravnenie-proizvoditelnosti-servinga-llama-3-na-vllm-lmdeploy-mlc-llm-tensorrt-llm-i-tgi?ysclid=m3fjcweeog473826955)).
Как будто TensorRT + Triton inference server сразу) При этом поддерживаются не только nvidia GPU.
Из объективного анализа фреймворков для сервинга привлекательнее всего выглядит именно vllm =)

### sglang

[Ссылка](https://github.com/sgl-project/sglang) (2024)

SGLang — фреймворк для работы с LLM и VLM с открытым исходным кодом. 
Основные функции включают:
 - Ускоренный бэкэнд: 
	 - кэширования префиксов (RadixAttention)
	 - декодирование с ограничением перехода вперед 
	 - непрерывное пакетирование
	 - token attention (paged attention), 
	 - тензорный параллелизм
	 - ядра FlashInfer
	 - chunked prefill
	 - квантование (INT4/FP8/AWQ/GPTQ)
- Интерфейс:
		- вызовы с цепочкой генерации
		- продвинутый промптинг
		- контроль потока
		- многомодальные входы
		- параллелизм
		- внешние взаимодействия
- Расширенная поддержка моделей: 
	- генеративные модели (Llama, Gemma, Mistral, QWen, DeepSeek, LLaVA и т. д.), 
	- встраиваемые модели (e5-mistral, gte, mcdse)
	- модели вознаграждения (Skywork)
	- + можно добавлять свои модели


### UniBench

[Github](https://github.com/facebookresearch/unibench) [Paper](https://arxiv.org/abs/2408.04810) (2024)

Набор инструментов и скриптов для оценки моделей VLM и бенчмарков. 
Включает:
- 60 VLM
- 40 оценочных бенчмарков
Можно добавлять свой скрипт и свою модель.
Большая часть UniBench распространяется по лицензии CC-BY-NC, однако некоторые части проекта доступны на условиях отдельной лицензии.

![](../files/Разработка%20ПО%20для%20бенчмарка%20VLLM-20241123.png)


### HELM

[Githib](https://github.com/stanford-crfm/helm) [Paper](https://arxiv.org/abs/2410.07112) (2023-2024)

Проект Holistic Evaluation of Language Models Стэнфордского CRFM. 
Пакет включает функции:
- сбор наборов данных в стандартном формате (например, NaturalQuestions)
- сбор моделей, доступных через унифицированный API (например, GPT-3, MT-NLG, OPT, BLOOM)
- сбор метрик, выходящих за рамки точности (efficiency, bias, toxicity и т. д.)
- сбор возмущений для оценки надежности и справедливости (например, опечатки, диалект)
- модульная структура для построения подсказок из наборов данных
- прокси-сервер для управления учетными записями и предоставления унифицированного интерфейса для доступа к моделям



### VL-CheckList

[Github](https://github.com/om-ai-lab/VL-CheckList) (2022)	

Фреймворк для оценки модели VLP (Vision-Language Pre-training). Текущий метод оценки заключается исключительно в сравнении ее точно настроенной производительности нижестоящих задач, что имеет ряд ограничений, таких как плохая интерпретируемость, несопоставимые результаты и предвзятость данных.

Основной принцип VL-CheckList: 
- оценка фундаментальных возможностей модели VLP вместо производительности в приложениях 
- разделение возможностей на относительно независимые переменные, которые легче анализировать

VL-CheckList оценивает модели VLP с трех сторон: 
- объект
- атрибут
- связь

Результат --> таблица производительности + радиарная диаграмма

![](../files/Разработка%20ПО%20для%20бенчмарка%20VLLM-20241123-1.png)


## Общая стратегия

1. Запускаем модель хоть как то =)    
2. Затем стараемся на vllm

## Ресерч

Антон
- тестировать запуск [VLMEvalKit](https://github.com/open-compass/VLMEvalKit)   

Семен
- Поискать свой бенчмарк для VLLM  
или
- Hugging Face datasets , vllm([ссылка](https://docs.vllm.ai/en/latest/) , [github](https://github.com/vllm-project/vllm))  
	1. Позволяет создать датасет	    
	2. Позволяет опубликовать свой датасет	    
	3. Инструменты для оценивания моделей на датасет.	    
	4. Умеет считать метрики WER и CER
или
- если не нашли то [VLMEvalKit](https://github.com/open-compass/VLMEvalKit)    

Юлия
- поискать свой бенчмарк для VLLM для VQA	
- тестировать его    
- если не нашли то [VLMEvalKit](https://github.com/open-compass/VLMEvalKit)    

Женя
- поискать свой бенчмарк для VLLM для VQA    
- тестировать его    
- если не нашли то [VLMEvalKit](https://github.com/open-compass/VLMEvalKit)    

Остальное:
- Велосипед от Антона 
[https://gitlab.com/document_vqa/vqa_dataset_bencmark](https://gitlab.com/document_vqa/vqa_dataset_bencmark)
